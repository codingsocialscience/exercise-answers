{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "website_content = requests.get(\"http://codingsocialscience.org/\").text\n",
    "\n",
    "page = BeautifulSoup ( website_content , 'html.parser' )\n",
    "\n",
    "for list in page.find_all('li'):\n",
    "    print( list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.2\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "website_content = requests.get(\"https://uk.sagepub.com/\").text\n",
    "\n",
    "page = BeautifulSoup ( website_content , 'html.parser' )\n",
    "\n",
    "image_count = 0\n",
    "\n",
    "for image in page.find_all('img'):\n",
    "    image_count = image_count + 1\n",
    "    \n",
    "print( image_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.3\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "website_content = requests.get(\"https://www.helsinki.fi/\").text\n",
    "website_content = website_content.lower()\n",
    "\n",
    "search_words = [ 'Twitter', 'Facebook', 'YouTube' ]\n",
    "\n",
    "for search_word in search_words:\n",
    "    if search_word.lower() in website_content:\n",
    "        print(\"Found\", search_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.4\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "universities = ['https://www.helsinki.fi', 'https://www.aalto.fi']\n",
    "results = {}\n",
    "\n",
    "search_words = [ 'Twitter', 'Facebook', 'YouTube' ]\n",
    "\n",
    "for university in universities:\n",
    "    website_content = requests.get( university ).text\n",
    "    website_content = website_content.lower()\n",
    "    \n",
    "    res = {} ## this is for storing service, count-pairs\n",
    "    for search_word in search_words:\n",
    "        res[ search_word ] = website_content.count( search_word.lower() )\n",
    "        \n",
    "    results[ university ] = res\n",
    "    \n",
    "print( results )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.5\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_url = \"http://codingsocialscience.org/\"\n",
    "\n",
    "start_page = requests.get( start_url ).text\n",
    "start_page = BeautifulSoup ( start_page , 'html.parser' )\n",
    "\n",
    "for link in start_page.find_all('a'):\n",
    "    link = link['href']\n",
    "    \n",
    "    ## there are both internal and external links, for simplicity let's identify external links as those starting with http\n",
    "    \n",
    "    if link.startswith('http'):\n",
    "        print( start_url + \"-\" + link )\n",
    "        followup = requests.get( link ).text\n",
    "        followup = BeautifulSoup( followup, 'html.parser' )\n",
    "        \n",
    "        for followuplink in followup.find_all('a'):\n",
    "            followuplink = followuplink['href']\n",
    "            if followuplink.startswith('http'):\n",
    "                print( link + \"-\" + followuplink )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.6\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'https://data.police.uk/api/crimes-street/all-crime?lat=51.5073&lng=-0.171505' ## latest month is shown by default, see documentation\n",
    "data = requests.get( url ).text ## could just be .json() for simplicyt, but doing here step by step\n",
    "data = json.loads( data )\n",
    "\n",
    "categories = {} ## this is a megacollector\n",
    "\n",
    "for entry in data:\n",
    "    \n",
    "    category = entry['category']\n",
    "    \n",
    "    if category not in categories:\n",
    "        categories[ category ] = 0\n",
    "        \n",
    "    categories[ category ] = categories[ category ] + 1\n",
    "    \n",
    "\n",
    "print( categories )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.7\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://api.worldbank.org/v2/country/fi?format=json'\n",
    "data = requests.get( url ).text ## could just be .json() for simplicyt, but doing here step by step\n",
    "data = json.loads( data )\n",
    "\n",
    "print( data[1][0]['capitalCity'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.8\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "adjacents = ['se', 'no', 'ru', 'ee']\n",
    "\n",
    "for country in adjacents:\n",
    "    url = 'http://api.worldbank.org/v2/country/' + country + '?format=json'\n",
    "    data = requests.get( url ).text ## could just be .json() for simplicy, but doing here step by step\n",
    "    data = json.loads( data )\n",
    "\n",
    "    print( data[1][0]['capitalCity'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.9\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://api.worldbank.org/v2/country/all?format=json'\n",
    "data = requests.get( url ).text ## could just be .json() for simplicy, but doing here step by step\n",
    "data = json.loads( data )\n",
    "\n",
    "## the book does not cover pagination, so let's this is not a full result\n",
    "\n",
    "data = data[1] ## index 0 is for pagination details\n",
    "\n",
    "for country in data:\n",
    "    print( country['latitude'], country['longitude'], country['incomeLevel']['value'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.10\n",
    "\n",
    "example = '127.0.0.1 - - [10/Nov/2020:08:43:18 +0200] \"GET / HTTP/1.1\" 200 4471 \"-\" \"Mozilla/5.0\"'\n",
    "example = example.split('[')[1] ## everything to right from [\n",
    "example = example.split(']')[0] ## everything to left from ]\n",
    "## 10/Nov/2020:08:43:18 +0200 # you can print it out here\n",
    "## print( example )\n",
    "example = example.split(':')[0] ## everything left from :\n",
    "date = example.split('/')\n",
    "day = date[0]\n",
    "month = date[1]\n",
    "year = date[2]\n",
    "\n",
    "print( year, month, day )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.11\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "all_text = ''\n",
    "\n",
    "reader = PdfReader('alice.pdf')\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "\n",
    "for i in range( number_of_pages ):\n",
    "    page = reader.pages[i]\n",
    "    text = page.extract_text()\n",
    "    all_text = all_text + text\n",
    "    \n",
    "print( all_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.12\n",
    "\n",
    "from docx import Document\n",
    "\n",
    "all_text = ''\n",
    "\n",
    "document = Document('alice.docx')\n",
    "\n",
    "for paragraph in document.paragraphs:\n",
    "    all_text = all_text + paragraph.text\n",
    "    \n",
    "print( all_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.13\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "universities = ['https://www.helsinki.fi', 'https://www.aalto.fi']\n",
    "results = {}\n",
    "\n",
    "search_words = [ 'Twitter', 'Facebook', 'YouTube' ]\n",
    "\n",
    "for university in universities:\n",
    "    website_content = requests.get( university ).text\n",
    "    website_content = website_content.lower()\n",
    "    \n",
    "    res = {} ## this is for storing service, count-pairs\n",
    "    for search_word in search_words:\n",
    "        res[ search_word ] = website_content.count( search_word.lower() )\n",
    "        \n",
    "    results[ university ] = res\n",
    "    \n",
    "import json\n",
    "\n",
    "json.dump( results, open(\"universities.json\", \"w\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.14\n",
    "\n",
    "import json\n",
    "\n",
    "data = json.load( open(\"universities.json\") )\n",
    "\n",
    "for university, results in data.items():\n",
    "    counts = 0\n",
    "    for service, count in results.items():\n",
    "        counts = counts + count\n",
    "    print( university, counts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.15\n",
    "\n",
    "import json\n",
    "\n",
    "## this could also be a dictionary in dictionary\n",
    "northwest = {}\n",
    "northeast = {}\n",
    "southwest = {}\n",
    "southeast = {}\n",
    "\n",
    "data = json.load( open(\"countries.json\") )\n",
    "\n",
    "for country in data:\n",
    "    income = country['income']\n",
    "    if country['lat'] > 0:\n",
    "        if country['long'] > 0:\n",
    "            if income not in northeast:\n",
    "                northeast[ income ] = 0\n",
    "            northeast[ income ] = northeast[ income ] + 1\n",
    "        else:\n",
    "            if income not in northwest:\n",
    "                northwest[ income ] = 0\n",
    "            northwest[ income ] = northwest[ income ] + 1\n",
    "    if country['lat'] < 0:\n",
    "        if country['long'] > 0:\n",
    "            if income not in southeast:\n",
    "                southeast[ income ] = 0\n",
    "            southeast[ income ] = southeast[ income ] + 1\n",
    "        else:\n",
    "            if income not in southwest:\n",
    "                southwest[ income ] = 0\n",
    "            southwest[ income ] = southwest[ income ] + 1\n",
    "            \n",
    "print( northwest, northeast, southwest, southeast )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.16\n",
    "\n",
    "import pandas\n",
    "\n",
    "url = 'https://data.police.uk/api/crimes-street/all-crime?lat=51.5073&lng=-0.171505' ## latest month is shown by default, see documentation\n",
    "dataframe = pandas.read_json(url)\n",
    "\n",
    "## note: it is also possible to manually construct the dataframe if you wish and append, remove rows from it. this is a simple example to make it clear why one might want to use tools such as pandas\n",
    "\n",
    "print( dataframe['category'].value_counts() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
